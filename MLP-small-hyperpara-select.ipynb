{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11fc1a87-fe13-4f04-a53f-de3889d3f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.layers import Input\n",
    "import copy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a130f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load datasets\n",
    "def get_dataset(obj_name):\n",
    "    filename_labels = 'train_labels.npy'\n",
    "    filename_origin = 'origin_cifar10.npy'\n",
    "    filename_small_noise = 'small_noise_cifar10.npy'\n",
    "    filename_medium_noise = 'medium_noise_cifar10.npy'\n",
    "    filename_large_noise = 'large_noise_cifar10.npy'\n",
    "    \n",
    "    train_set = None\n",
    "    match obj_name:\n",
    "        case 'origin':\n",
    "            train_set = np.load(filename_origin)\n",
    "        case 'small_noise':\n",
    "            train_set = np.load(filename_small_noise)\n",
    "        case 'medium_noise':\n",
    "            train_set = np.load(filename_medium_noise)\n",
    "        case 'large_noise':\n",
    "            train_set = np.load(filename_large_noise)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown obj_name: {obj_name}\")\n",
    "    label_set = np.load(filename_labels) if train_set.size > 0 else None\n",
    "\n",
    "    if label_set is None or train_set is None:\n",
    "        print(\"Run Noise_process.ipynb before running this file\")\n",
    "    return train_set, label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0870639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build MLP model\n",
    "def build_mlp_model(input_shape, num_classes, learning_rate=0.0005, num_units=128, dropout_rate=0):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Flatten(),\n",
    "        Dense(num_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e919b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process dataset\n",
    "def get_processed_dataset(model_name):\n",
    "    train_set, label_set = get_dataset(model_name)\n",
    "    X_combined_reshaped = train_set.reshape(-1, 32, 32, 3)\n",
    "    X_combined_reshaped = X_combined_reshaped.astype('float32') / 255.0\n",
    "    y_combined_categorical = to_categorical(label_set, num_classes=10)\n",
    "    return X_combined_reshaped, y_combined_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e95609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "ORIGIN = 'origin'\n",
    "SMALL = 'small_noise'\n",
    "MEDIUM = 'medium_noise'\n",
    "LARGE = 'large_noise'\n",
    "all_dataset = [ORIGIN, SMALL, MEDIUM, LARGE]\n",
    "unit_sizes = [32, 64, 128, 256, 512]\n",
    "dropout_rate = 0  # Dropout rate can be adjusted as needed\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53a97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer cross-validation setup\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_params = {}\n",
    "performance_results = []\n",
    "\n",
    "# Assuming you have your dataset ready from the previous function\n",
    "x_combined, y_combined = get_processed_dataset(MEDIUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908f31c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3262 - loss: 1.8992\n",
      "Final performance:  0.33916667103767395\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3501 - loss: 1.8173 \n",
      "Final performance:  0.3499999940395355\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3727 - loss: 1.7489  \n",
      "Final performance:  0.38499999046325684\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3362 - loss: 1.8732  \n",
      "Final performance:  0.34333333373069763\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.3422 - loss: 1.8399\n",
      "Final performance:  0.33916667103767395\n",
      "Average Performance across all outer folds: 0.3513333320617676\n",
      "Best Parameters for each fold: {8: 256, 12: 256, 0: 512, 1: 512, 3: 256}\n"
     ]
    }
   ],
   "source": [
    "#Outer cross-validation loop\n",
    "for outer_train_idx, outer_val_idx in outer_cv.split(x_combined):\n",
    "    X_outer_train, X_outer_val = x_combined[outer_train_idx], x_combined[outer_val_idx]\n",
    "    y_outer_train, y_outer_val = y_combined[outer_train_idx], y_combined[outer_val_idx]\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_num_units = None\n",
    "\n",
    "    # Inner cross-validation to find the best unit size\n",
    "    for num_units in unit_sizes:\n",
    "        inner_cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "\n",
    "        for inner_train_idx, inner_val_idx in inner_cv.split(X_outer_train):\n",
    "            X_inner_train, X_inner_val = X_outer_train[inner_train_idx], X_outer_train[inner_val_idx]\n",
    "            y_inner_train, y_inner_val = y_outer_train[inner_train_idx], y_outer_train[inner_val_idx]\n",
    "\n",
    "            # Build and train the MLP model\n",
    "            model = build_mlp_model(input_shape=(32, 32, 3), num_classes=10, learning_rate=learning_rate, num_units=num_units, dropout_rate=dropout_rate)\n",
    "            history = model.fit(\n",
    "                x=X_inner_train,\n",
    "                y=y_inner_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=num_epochs,\n",
    "                validation_data=(X_inner_val, y_inner_val),\n",
    "                verbose=0\n",
    "            )\n",
    "            #print(history.history)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            score = model.evaluate(X_inner_val, y_inner_val, verbose=0)[1]\n",
    "            scores.append(score)\n",
    "\n",
    "        # Get the mean score from the inner cross-validation\n",
    "        mean_score = np.mean(scores)\n",
    "\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_num_units = num_units\n",
    "\n",
    "    # Store the best parameters for the outer fold\n",
    "    best_params[outer_val_idx[0]] = best_num_units\n",
    "\n",
    "    # Build final model using the best number of units\n",
    "    final_model = build_mlp_model(input_shape=(32, 32, 3), num_classes=10, learning_rate=learning_rate, num_units=best_num_units, dropout_rate=dropout_rate)\n",
    "    final_model.fit(X_outer_train, y_outer_train, epochs=num_epochs, batch_size=batch_size, verbose=0)\n",
    "    final_performance = final_model.evaluate(X_outer_val, y_outer_val)[1]\n",
    "    print(\"Final performance: \", final_performance)\n",
    "    performance_results.append(final_performance)\n",
    "\n",
    "# Calculate and display the average performance across all outer folds\n",
    "average_performance = np.mean(performance_results)\n",
    "print(f'Average Performance across all outer folds: {average_performance}')\n",
    "print(f'Best Parameters for each fold: {best_params}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
