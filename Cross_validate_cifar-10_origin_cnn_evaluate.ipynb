{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Noise_process before run this file\n",
    "def get_dataset(obj_name):\n",
    "    filename_labels = 'train_labels.npy'\n",
    "    filename_origin = 'origin_cifar10.npy'\n",
    "    filename_small_noise = 'small_noise_cifar10.npy'\n",
    "    filename_medium_noise = 'medium_noise_cifar10.npy'\n",
    "    filename_large_noise = 'large_noise_cifar10.npy'\n",
    "    \n",
    "    train_set = None\n",
    "    match obj_name:\n",
    "        case 'origin':\n",
    "            train_set = np.load(filename_origin)\n",
    "        case 'small_noise':\n",
    "            train_set = np.load(filename_small_noise)\n",
    "        case 'medium_noise':\n",
    "            train_set = np.load(filename_medium_noise)\n",
    "        case 'large_noise':\n",
    "            train_set = np.load(filename_large_noise)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown obj_name: {obj_name}\")\n",
    "    label_set = np.load(filename_labels) if train_set.size > 0 else None\n",
    "\n",
    "    if label_set is None or train_set is None:\n",
    "        print(\"Run Noise_process.ipynb before run this file\")\n",
    "    return train_set, label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGIN='origin'\n",
    "SMALL='small_noise'\n",
    "MEDIUM='medium_noise'\n",
    "LARGE='large_noise'\n",
    "all_dataset = [ORIGIN, SMALL, MEDIUM, LARGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN: learning_rate, num_filter, filter_size\n",
    "best_config_origin_cnn={'name': ORIGIN, 'num_filter': 64}\n",
    "best_config_small_cnn={'name': SMALL, 'learning_rate': 0.001, 'num_filter': 32, 'filter_size': (3,3)}\n",
    "best_config_medium_cnn={'name': MEDIUM, 'learning_rate': 0.0005, 'num_filter': 32, 'filter_size': (3,3)}\n",
    "best_config_large_cnn={'name': LARGE, 'learning_rate': 0.0005, 'num_filter': 128, 'filter_size': (3,3)}\n",
    "best_config_cnn=[]\n",
    "best_config_cnn.append(best_config_origin_cnn)\n",
    "best_config_cnn.append(best_config_small_cnn)\n",
    "best_config_cnn.append(best_config_medium_cnn)\n",
    "best_config_cnn.append(best_config_large_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_cnn(num_filter, learning_rate=0.0005, filter_size=(3,3), input_shape=(32, 32, 3), num_classes=10):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))  \n",
    "    model.add(layers.Conv2D(num_filter, filter_size, activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_dataset(model_name):\n",
    "    train_set, label_set = get_dataset(model_name)\n",
    "\n",
    "    X_combined_reshaped = train_set.reshape(-1, 32, 32, 3)\n",
    "    X_combined_reshaped = X_combined_reshaped.astype('float32') / 255.0\n",
    "    y_combined_categorical = to_categorical(label_set, num_classes=10)\n",
    "\n",
    "    return X_combined_reshaped, y_combined_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, model_name):\n",
    "    plt.figure(figsize=(24, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Loss Curve {model_name} CNN')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy Curve {model_name} CNN')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_performance_cnn = {}\n",
    "all_histories = {}\n",
    "for config in best_config_cnn:\n",
    "    model_name = config['name']\n",
    "    model = build_simple_cnn(config['learning_rate'], config['num_filter'], config['filter_size'])\n",
    "\n",
    "    model_performance = {}\n",
    "    \n",
    "    # Use 80% for cross validation and 20% for robustness test for other model\n",
    "    x_processed, y_processed = get_processed_dataset(model_name)\n",
    "    x_cv, _, y_cv, _ = train_test_split(x_processed, y_processed, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Cross Validation\n",
    "    kf = KFold(n_splits=20, shuffle=True, random_state=42)\n",
    "    all_history = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(x_cv)):\n",
    "        print(f'Fold {fold + 1}/{20}')\n",
    "\n",
    "        x_train, x_val = x_cv[train_index], x_cv[val_index]\n",
    "        y_train, y_val = y_cv[train_index], y_cv[val_index]\n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "        all_history.append(history.history)\n",
    "\n",
    "    plot_learning_curves(all_history, model_name)\n",
    "    all_histories[model_name] = all_history\n",
    "\n",
    "    # Release Memory Space\n",
    "    del x_processed, y_processed, x_cv, y_cv\n",
    "\n",
    "    # Robustness Test\n",
    "    for dataset_name in all_dataset:\n",
    "        if dataset_name == model_name:\n",
    "            # Do robustness evaluations on other dataset\n",
    "            continue\n",
    "        # Get rest part of dataset which never used on Cross Validation\n",
    "        x_evaluated, y_evaluated = get_processed_dataset(dataset_name)\n",
    "        _, x_test, _, y_test = train_test_split(x_evaluated, y_evaluated, test_size=0.2, random_state=42)\n",
    "\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_true_classes = np.argmax(y_test, axis=1) \n",
    "\n",
    "        accuracy = accuracy_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "        precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "        print(f\"Run {model_name}, in {dataset_name}: Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n",
    "        model_performance[dataset_name] = {'accuracy' : accuracy, 'precision' : precision, 'recall' : recall}\n",
    "    all_performance_cnn[model_name] = model_performance\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
